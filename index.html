
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-67835289-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-67835289-3');
</script>

<!-- ======================================================================= -->
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<style type="text/css">
  body {
    /* font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; */
    font-family: "Avenir", sans-serif;
    font-weight:400;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
  }

  h1 {
    font-weight:300;
  }

  .column {
    float: left;
    /* width: 50%; */
  }

  /* Clear floats after the columns */
  .row:after {
    content: "";
    display: table;
    clear: both;
  }
  
  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  #authors td {
    padding-bottom:5px;
    padding-top:30px;
  }
</style>


<html>
  <head>

	<title>Turbulence Mitigation Transformer</title>
	<meta property="og:title" content="Turbulence Mitigation Transformer" />
	<meta property="og:image" content="" />
	<meta property="og:image:width" content="3000" />
	<meta property="og:image:height" content="800" />
  </head>

  <body>
    <br><br><br><br>
          <center>
          	<span style="font-size:38px">Turbulence Mitigation Transformer</span>
            <br><br>
		  <table align=center width=800px>
	  			  <tr>
	  	        <td align=center width=100px>
	  					  <center>
	  						<span style="font-size:24px"><a href="http://xg416.github.io">Xingguang Zhang</a></span>
		  		  		</center>
		  		  	</td>
	  	        <td align=center width=100px>
                <center>
	  						<span style="font-size:24px"><a href="https://web.ics.purdue.edu/~mao114/">Zhiyuan Mao</a></span>
		  		  		</center>
		  		  	</td>
	  	        <td align=center width=100px>
	  					  <center>
	  						<span style="font-size:24px"><a href="https://web.ics.purdue.edu/~nchimitt/index.html">Nicholas Chimitt</a></span>
		  		  		</center>
		  		  	</td>
	  	        <td align=center width=100px>
	  					  <center>
	  						<span style="font-size:24px"><a href="https://engineering.purdue.edu/ChanGroup/stanleychan.html">Stanley Chan</a></span>
		  		  		</center>
		  		  	</td>
			  </table>

	  		  <table align=center width=650px style="font-size:20px;">
	  			  <tr>
              <td align=center width=150px>
                <center>
                  <span><a href="http://arxiv.org/abs/2207.06465"> [Paper]</a></span>
                  </center>
                  </td>
	  	          <td align=center width=150px>
	  					<center>
	  						<span style=""><a href="https://github.com/xg416/TMT"> [Code]</a></span>
		  		  		</center>
		  		  	  </td>
                <td align=center width=150px>
	  					<center>
	  						<span style=""><a href="https://engineering.purdue.edu/ChanGroup/project_turbulence.html"> [Relevant]</a></span>
		  		  		</center>
		  		  	  </td>
	  	          <td align=center width=200px>
                  <center>
                    <span style=""><a href="https://engineering.purdue.edu/ChanGroup/project_turbulence_TurbSim_v2.html"> [P2S Simulator]</a></span>
                    </center>
                  </td>
              </tr>
	  			  <tr>
			  </table>
        <!-- ======================================================================= <br>
        <span style="font-size:20px">under review</span> -->
      </center>

  		  <br><br>
  		  <table align=center width=1100px>
  			  <tr>
            <td width=1050px>
    					<center>
    	          <a href="figs/video_22.gif"><img src = "figs/video_22.gif" width="85%"></img></href></a><br>
  					  </center>
            </td>
          </tr>
  		  </table>
      	<br><br>

		  <hr>

      <!--  <table align=center>
        <div class='row'>
          <div class='column'>
            <center>
              <h1>Abstract</h1>
              <p style="width:560px; text-align:left; color:darkslategray; ;">This paper proposes a simple self-supervised approach for learning representations
                for visual correspondence from raw video. We cast correspondence as link prediction in a space-time graph constructed from a video. In this graph, the nodes
                are patches sampled from each frame, and nodes adjacent in time can share a
                directed edge. We learn a node embedding in which pairwise similarity defines
                transition probabilities of a random walk. Prediction of long-range correspondence
                is efficiently computed as a walk along this graph. The embedding learns to guide
                the walk by placing high probability along paths of correspondence. Targets are
                formed without supervision, by cycle-consistency: we train the embedding to maximize the likelihood of returning to the initial node when walking along a graph
                constructed from a 'palindrome' of frames. We demonstrate that the approach allows
                for learning representations from large unlabeled video. Despite its simplicity,
                the method outperforms the self-supervised state-of-the-art on a variety of label
                propagation tasks involving objects, semantic parts, and pose. Moreover, we show
                that self-supervised adaptation at test-time and edge dropout improve transfer for
                object-level correspondence.</p>
            </center>
          </div>

          <div class="column" style='float:right; width:40%'>
            <center>
              <h1>Pseudo-code</h1>
              <img src = "figs/pseudocode.png" width="90%"></img>
            </center>
          </div>
        </div>
      </table>
      <br><br>
    </table> -->

      <center>
        <h1>Abstract</h1>
        <p> Restoring images distorted by atmospheric turbulence is a ubiquitous problem in long-range imaging applications. While existing deep-learning based methods have demonstrated promising results in specific testing conditions, they suffer from three limitations: (1) lack of generalization capability from synthetic training data to real turbulence data; (2) fail to scale, hence causing memory and speed challenges when extending the idea to a large number of frames; (3) lack of a fast and accurate simulator to generate data for training neural networks.
          <br>
          In this paper, we introduce the turbulence mitigation transformer (TMT) that specifically aims to resolve these issues. TMT brings three contributions: Firstly, TMT explicitly uses turbulence physics by decoupling the turbulence degradation and introducing a multi-scale loss for removing distortion, thus improving effectiveness. Secondly, TMT presents a new attention module along the temporal axis to extra features efficiently, thus improving memory and speed. Thirdly, TMT introduces a new simulator based on the Fourier sampler, temporal correlation, and flexible kernel size, thus improving our capability to synthesize better training data.
          TMT outperforms state-of-the-art video restoration models, especially in generalizing from synthetic to real turbulence data.</p>
        <br><br>
		  </table>

		  <hr>

      <center>
        <h1>Results on real world static image sequences</h1>
        <table align=center width=1100px>
          <tr>
            <td align=center width=300px>
              <center>
                <a href="figs/pattern9.gif"><img src = "figs/pattern9.gif" width="85%"></img></href></a><br>
              </center>
            </td>
            <td align=center width=300px>
              <center>
              <a href="figs/pattern16.gif"><img src = "figs/pattern16.gif" width="85%"></img></href></a><br>
              </center>
            </td>
            <td align=center width=300px>
              <center>
              <a href="figs/pattern15.gif"><img src = "figs/pattern15.gif" width="85%"></img></href></a><br>
              </center>
            </td>
        </table>


        <table align=center width=1100px>
          <tr>
            <td align=center width=540px>
              <center>
                <a href="figs/pattern13.gif"><img src = "figs/pattern13.gif" width="90%"></img></href></a><br>
              </center>
            </td>
            <td align=center width=540px>
              <center>
              <a href="figs/pattern14.gif"><img src = "figs/pattern14.gif" width="90%"></img></href></a><br>
              </center>
            </td>
        </table>
        <p> Source of input images: 12 frames of the pattern 9, 13, 14, 15, 16 in the <a href="https://zenodo.org/communities/otis/?page=1&size=20"> OTIS</a> dataset </p>
      </center>
      <table align=center width=1080px>
        <tr>
          <td width=1080px>
            <center>
              <a href="figs/hillhouse.gif"><img src = "figs/hillhouse.gif" width="97%"></img></href></a><br>
            </center>
          </td>
        </tr>
        <br><br>
      </table>
      <p> Source of input images: first 12 frames of the hillhouse sequence in the <a href="https://arxiv.org/abs/2204.06989"> CLEAR's </a> dataset </p>
      <center>
        <table align=center width=1100px>
          <tr>
            <td align=center width=540px>
              <center>
                <a href="figs/pg2.gif"><img src = "figs/pg2.gif" width="90%"></img></href></a><br>
              </center>
            </td>
            <td align=center width=540px>
              <center>
              <a href="figs/pg24.gif"><img src = "figs/pg24.gif" width="90%"></img></href></a><br>
              </center>
            </td>
            <br><br>
        </table>
        <table align=center width=1100px>
          <tr>
            <td align=center width=540px>
              <center>
                <a href="figs/pg58.gif"><img src = "figs/pg58.gif" width="90%"></img></href></a><br>
              </center>
            </td>
            <td align=center width=540px>
              <center>
              <a href="figs/pg96.gif"><img src = "figs/pg96.gif" width="90%"></img></href></a><br>
              </center>
            </td>
            
        </table>
        <p> Source of input images: first 12 frames of the 2nd, 24th, 58th, 96th sequences in the <a href="http://cvpr2022.ug2challenge.org/dataset22_t3.html"> static text </a> dataset </p>
        <br><br>
      </center>

      <hr>


      <center>
      <h1>Results on real world dynamic image sequences</h1>
		  <table align=center width=1080px>
        <tr>
          <td align=center width=200px>
            <center>
            <span style="font-size:18px">Left: input sequence</span>
            </center>
          </td>
          <td align=center width=200px>
            <center>
            <span style="font-size:18px">Right: restored</span>
            </center>
          </td>
      </table>

      <table border="0" align="center" cellspacing="0" cellpadding="20">
        <video width="1080" height="320" controls muted>
          <source src="video/sh.mp4" type="video/mp4">
          <source src="video/sh.ogg" type="video/ogg">
        </video>
        <p> Source of the input video: the <a href="https://arxiv.org/abs/2204.06989"> CLEAR's </a> dataset </p>
        <br><br>
      </table>
      

      <table border="0" align="center" cellspacing="0" cellpadding="20">
        <video width="1080" height="320" controls muted>
          <source src="video/TMT_airport.mp4" type="video/mp4">
          <source src="video/TMT_airport.ogg" type="video/ogg">
        </video>
        <p> Source of the input video: the <a href="https://arxiv.org/abs/2204.06989"> CLEAR's </a> dataset </p>
      <br><br>
      </table>
      <table border="0" align="center" cellspacing="0" cellpadding="20">
        <video width="1080" height="263" controls muted>
          <source src="video/video_19.mp4" type="video/mp4">
          <source src="video/video_19.ogg" type="video/ogg">
        </video>
        <p> Source of the input video: the <a href="https://zenodo.org/record/5101910#.Ys2ak-yZOrc"> TSRWGAN's </a> dataset </p>
      <br><br>
      </table>
      <table border="0" align="center" cellspacing="0" cellpadding="20">
        <video width="1080" height="340" controls muted>
          <source src="video/video_23.mp4" type="video/mp4">
          <source src="video/video_23.ogg" type="video/ogg">
        </video>
        <p> Source of the input video: the <a href="https://zenodo.org/record/5101910#.Ys2ak-yZOrc"> TSRWGAN's </a> dataset </p>
      <br><br>
      </table>
      </center>

      <hr>
  		  <!-- <table align=center width=550px> -->
  		  <table align=center width=1100px>
	 		<center><h1>Network Scheme</h1></center>
       <center><span class="image"><img src="figs/model.png" alt="" width="1000" height="254"/></span></center>
       <br><br>
  		  </table>
		  <br>

			</left>
		</td>
			 </tr>
		</table>

		<br><br>


</body>
</html>
